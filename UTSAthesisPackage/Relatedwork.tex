\chapter{Related Work}
\label{chap:relatedwork}

\section{Empirical Studies of Performance Bugs}
\label{sec:empperformance}
Since performance is one of the critical non-functional requirements of software systems, a number of studies analyzed the performance bug of software systems. Zaman et al.~\cite{zaman_security_2011} did a bug analysis of generic bugs collected from Firefox web browser. Their study focused on security and performance bugs of Firefox web browser. Their followup work~\cite{Zaman_study_2012} analyzed the bug reports of Firefox and Chrome to differentiate characteristics performance and non-performance bugs. To identify performance bug code patterns, Jin et al.~\cite{Jin_performance_2012} did root cause analysis of 109 performance bug collected from five projects. Nistor et al.~\cite{Nistor_performance_2013} did a study on performance and non-performance bugs from three popular codebases: Eclipse JDT, Eclipse SWT, and Mozilla. Recent study~\cite{Chen:perf:19} on 700 performance bug fixing commits across 13 popular open-source projects characterizes the relative frequency of performance bug types as well as their complexity. Prior works on performance bugs focused on generic performance bug categorization. But in our study, we tried to focus on Unity performance bug taxonomy and complexity that can be helpful for developers and further research on identifying performance bugs.


\section{Empirical Studies of Generic Bugs}
\label{sec:genericbug}
There are several research works~\cite{Fonseca2010, Lu2008, Sahoo2010} that study and characterize different aspects of generic bugs. Park et al.~\cite{Park2012} did a study focusing on bugs that need more than one fix attempt. Asaduzzaman et al. ~\cite{Asaduzzaman2012} mined the bug introducing changes in the Android platform to identify problematic changes. Aranda et al. ~cite{Aranda2009} did a study on common bug fixing coordination patterns and to provide implications for software engineering tool developers.


\section{Performance Bug Detection}
\label{sec:detection}
Since performance issues are difficult to detect, several research works focused on identifying performance bugs. Several techniques~\cite{Siegmund2012, Killian2010, Yan2012} adopted to detect performance bugs. Recent work~\cite{Mostafa2017} focused on regression performance test case selection to detect bugs early. Han et al.~\cite{Han2018} utilize Machine Learning to generate test frames for guiding actual performance test case generation. While Chen et al.~\cite{Chen2014} discussed performance Anti-Patterns for applications.


\section{Performance Optimization of Game Applications}
\label{sec:perfgame}
Since Game applications constantly use CPUs and GPUs for frame rendering, the research community focused on optimizing the rendering workload to get a satisfactory performance. DJay~\cite{Grizan15} utilized a predictive SSIM-based approach to tune GPU workloads. The system used a cloud gaming server that defines cost in terms of GPU time and benefits in rendering quality. Apogee~\cite{Sethia_apogee:adaptive} utilizes a low-overhead caching approach that adapts to address patterns found in graphics memory. Kwon et al.~\cite{Kwon2017} proposed a streaming-based execution offloading framework to maximize the uses of computational resources to improve graphics rendering. Vatjus-Anttila et al.~\cite{Vatjus2013} built a power model based on rendering complexity such as number of triangles, render batches etc. RAVEN~\cite{Hwang17} utilizes human visual perception of graphics changes to reduce power uses without degrading user experiences. Prior works mostly focused on graphics rendering and power consumption optimization to improve game applications. In our research, we focused on code optimization to improve rendering based applications such as AR, VR, or Game applications. 
